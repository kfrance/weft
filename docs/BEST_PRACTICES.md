# Best Practices

- **Keep tests in the default run**: Avoid adding `pytest` markers whose only purpose is to skip tests because a dependency might be missing or the test could be slow. Keep those tests in the normal `pytest` run instead.
- **Use pytest.fail() for missing dependencies, not pytest.skip()**: When a test requires an external dependency that might not be available, use `pytest.fail()` with a clear error message instead of `pytest.skip()`. This ensures developers are notified of missing dependencies rather than silently skipping tests. Example: `pytest.fail("Required dependency 'droid' not found. Install it first with: pip install droid-cli")`.
- **Avoid mocking DSPy and LLMs**: Use real DSPy components with real LLM API calls in tests. Configure tests with actual LLM providers (e.g., OpenRouter) rather than creating mock LLM objects or stub responses. DSPy's caching ensures the first test run hits the API while subsequent runs retrieve cached results, making tests both fast and representative of production behavior.
- **Documentation is verified manually**: Avoid writing tests whose only purpose is to check for the existence of documentation pages or sectionsâ€”keep effort focused on code behavior.
