---
plan_id: train-command-v1
status: done
evaluation_notes: []
git_sha: 193e27460d315f6820667dfcca60fc8fb22974bf
---

## Objectives

Implement a `train` command that loads training data generated by the `eval` command, analyzes it using DSPy, and generates candidate prompt sets to improve the AI coding workflow. This is the first step toward a self-optimizing prompt system.

## Requirements & Constraints

### Functional Requirements

1. **Training data loading**:
   - Load 1-3 complete training samples per batch from `.lw_coder/training_data/<plan_id>/`
   - Each sample contains: plan.md, code_trace.md, human_feedback.md, judge results (JSON), test results (JSON)
   - Batch size is configurable (default: 3)
   - Handle missing optional files gracefully (code_trace.md, test_results_before.json)

2. **Current prompt loading**:
   - Load current best prompts from `.lw_coder/prompts/active/claude-code-cli/<model>/`
   - Load main.md and all subagent .md files

3. **DSPy-based prompt generation**:
   - Use a single DSPy signature (`PromptTrainer`) with comprehensive instructions
   - Generate improved main prompt and up to 5 subagents (configurable, default: 5)
   - Subagent names and roles are dynamic based on training insights
   - Track and report token usage (input/output/total)

4. **Candidate output**:
   - Save generated prompts to `.lw_coder/prompts/candidates/claude-code-cli/<model>/candidate-NNN/`
   - Sequential numbering (candidate-001, candidate-002, etc.)
   - Each candidate directory contains: main.md + subagent .md files
   - No auto-promotion (separate future feature)

5. **Directory structure migration**:
   - Migrate existing `optimized_prompts/` to `prompts/active/`
   - Update `load_prompts()` to check new location first, fall back to old with migration
   - Migration moves contents and deletes old `optimized_prompts/` directory
   - Update `init` command to install to `prompts/active/`

### Technical Constraints

- Claude Code only (Droid support out of scope)
- DSPy with OpenRouter for LLM calls (consistent with judge executor pattern)
- Use DSPy caching to minimize API costs on repeated runs
- No manifest.json files - just markdown files following existing patterns

### Quality Requirements

- All existing tests must continue to pass
- New functionality must have comprehensive unit and integration test coverage
- Code must follow existing project patterns and conventions

## Work Items

### 1. Create Pydantic models for training data

**File**: `src/lw_coder/training_types.py`

Define custom types for DSPy signature:

```python
from pydantic import BaseModel

class TrainingSample(BaseModel):
    """A complete training sample from eval command."""
    plan_id: str
    plan_content: str
    code_trace: str  # May be empty if not captured
    human_feedback: str
    judge_results: str  # Formatted string of all judge scores/feedback
    test_results_before: str  # JSON string, may be empty
    test_results_after: str   # JSON string

class SubagentDefinition(BaseModel):
    """Definition of a subagent for the coding workflow."""
    name: str           # kebab-case, e.g., "code-review-auditor"
    description: str    # Brief description for YAML frontmatter
    prompt: str         # Full markdown prompt content

class CurrentPrompts(BaseModel):
    """The current best prompts being improved upon."""
    main_prompt: str
    subagents: list[SubagentDefinition]

class CandidatePrompts(BaseModel):
    """Output: A new candidate prompt set."""
    main_prompt: str
    subagents: list[SubagentDefinition]
    analysis_summary: str  # What was learned and improved
```

### 2. Create training data loader

**File**: `src/lw_coder/training_data_loader.py`

Functions to load training samples:

```python
def discover_training_samples(repo_root: Path) -> list[str]:
    """Discover available training sample plan_ids."""

def load_training_sample(repo_root: Path, plan_id: str) -> TrainingSample:
    """Load a complete training sample by plan_id."""

def load_training_batch(repo_root: Path, batch_size: int = 3) -> list[TrainingSample]:
    """Load a batch of training samples."""
```

- Load from `.lw_coder/training_data/<plan_id>/`
- Required files: human_feedback.md, test_results_after.json, at least one judge JSON
- Optional files: code_trace.md, test_results_before.json
- Format judge results into readable string for DSPy input

### 3. Create DSPy prompt trainer module

**File**: `src/lw_coder/prompt_trainer.py`

DSPy signature and execution:

```python
class PromptTrainerSignature(dspy.Signature):
    """Analyze training data to generate improved prompts."""

    training_samples: list[TrainingSample] = dspy.InputField()
    current_prompts: CurrentPrompts = dspy.InputField()
    max_subagents: int = dspy.InputField()

    candidate: CandidatePrompts = dspy.OutputField()

def run_prompt_trainer(
    training_samples: list[TrainingSample],
    current_prompts: CurrentPrompts,
    max_subagents: int,
    model: str,
    api_key: str,
    cache_dir: Path,
) -> tuple[CandidatePrompts, dict[str, int]]:
    """Run the prompt trainer and return candidate + token usage."""
```

The signature should use `.with_instructions()` to add the following prompt instructions:

```markdown
You are an expert prompt engineer specializing in AI coding assistants. Your task is to analyze training data from real coding sessions and generate improved prompts.

## How Claude Code CLI Works

Claude Code is a CLI tool that executes coding tasks. When given a task:
1. It receives a main prompt that defines its behavior and workflow
2. It can invoke subagents by name - these are specialized helpers with their own prompts
3. It has access to tools: Read (files), Grep (search), Bash (commands), Edit (modify files), etc.
4. Subagents inherit tool access from the parent agent

The main prompt typically defines:
- The overall workflow (read plan, implement, review loop)
- When and how to invoke subagents
- Operating principles and constraints

Subagents are invoked by name and return their analysis to the main agent. Each subagent has:
- A name (e.g., "code-review-auditor")
- A focused responsibility
- Its own prompt defining how it analyzes and responds

## Key Principles for Effective AI Coding Prompts

### 1. Verification is Easier Than Creation

LLMs, like humans, are better at evaluating work than producing it. It's easier to review code and identify issues than to write correct code from scratch. This has important implications:

- Build feedback loops into the workflow. Don't just write code—write, then verify, then fix.
- Subagents are excellent for verification roles (reviewers, checkers, validators) because they bring fresh, focused context to the judgment task.
- Multiple verification passes from different angles catch more issues than a single review.
- The main agent benefits from verification feedback to converge on the best result.

When designing prompts, ensure the workflow includes explicit verification steps. A prompt that says "implement and move on" will produce worse results than one that says "implement, then invoke reviewers, then address their feedback."

### 2. Context Preservation Through Delegation

The more context an AI accumulates, the more likely it is to make mistakes or lose focus. Context is a finite, precious resource. Prompts should actively preserve context through strategic delegation:

**When to use subagents:**
- Verification/review tasks (fresh context = better judgment)
- Exploration/research (use the Explore tool—it returns summaries without polluting your context with all files read)
- Focused analysis from a specific angle
- Any task that benefits from isolated, dedicated attention
- When you need to read many files or do extensive analysis—delegate and get back conclusions

**When NOT to split across subagents:**
- Implementation tasks that must coordinate (shared interfaces, naming conventions, function signatures)
- Work where one piece depends on decisions made in another piece
- Anything where the agents would need to "talk to each other" to succeed

**The principle:** Subagents are excellent for parallel, independent work. They struggle with tightly-coupled, interdependent work that requires coordination.

**Think of it like:** A lead developer who delegates investigation and review work to team members. They do the deep dive and report back findings. The lead makes decisions based on their summaries, not by reading everything themselves. After receiving subagent results, you get their insights without inheriting their context burden.

Proactively delegate to subagents to keep your own context clean. Don't wait until you're overwhelmed—plan for delegation from the start.

## Your Analysis Process

1. **Understand the Human Feedback First**
   This is the most critical input. The human who ran the coding session tells you what went wrong or right. Common issues include:
   - Agent skipped over problems instead of fixing them
   - Agent didn't complete all the work
   - Agent made changes that weren't requested
   - Agent got stuck in loops
   - Agent produced good results (learn from successes too)

2. **Examine the Code Trace**
   The trace shows the actual conversation: what the agent did, what tools it called, what it produced. Look for:
   - Where did the agent deviate from ideal behavior?
   - What patterns led to the issues mentioned in human feedback?
   - What worked well that should be reinforced?

3. **Review Judge Results**
   Judges score specific aspects (code reuse, plan compliance, etc.). Low scores indicate areas needing improvement. High scores indicate successful patterns.

4. **Review Test Results**
   Compare before/after test results. Did tests pass? Did new tests get added? Were existing tests broken?

5. **Synthesize Improvements**
   Based on your analysis:
   - Identify specific prompt weaknesses that led to problems
   - Design targeted improvements to address those weaknesses
   - Preserve what's working well in the current prompts
   - Consider whether new subagents could help (e.g., a "completion-checker" if work often goes unfinished)

## Output Requirements

Generate a candidate prompt set that:
1. Addresses the specific issues found in training data
2. Preserves successful patterns from current prompts
3. Defines up to {max_subagents} subagents (only create what's needed)
4. Uses clear, actionable language
5. Includes specific examples where helpful

For each subagent, provide:
- A descriptive name (kebab-case, e.g., "test-validator")
- A brief description of its responsibility
- A complete prompt that enables it to do its job effectively

In your analysis_summary, explain:
- What issues you identified from the training data
- What specific changes you made to address them
- Why you chose the subagents you defined
```

### 4. Update prompt loader for new directory structure

**File**: `src/lw_coder/prompt_loader.py`

Modify `load_prompts()`:
- Check `prompts/active/` first
- If not found but `optimized_prompts/` exists, migrate:
  - Move contents to `prompts/active/`
  - Delete `optimized_prompts/` directory
  - Log migration message
- Return prompts from new location

Add new function:
```python
def load_current_prompts_for_training(
    repo_root: Path,
    tool: str = "claude-code-cli",
    model: str = "sonnet",
) -> CurrentPrompts:
    """Load current prompts as CurrentPrompts object for training."""
```

This loads main.md and discovers all subagent .md files in the directory.

### 5. Create candidate writer

**File**: `src/lw_coder/candidate_writer.py`

Functions to write candidate prompt sets:

```python
def get_next_candidate_number(repo_root: Path, tool: str, model: str) -> int:
    """Get the next sequential candidate number."""

def write_candidate(
    repo_root: Path,
    tool: str,
    model: str,
    candidate: CandidatePrompts,
) -> Path:
    """Write a candidate prompt set and return the directory path."""
```

- Creates `.lw_coder/prompts/candidates/<tool>/<model>/candidate-NNN/`
- Writes main.md with candidate.main_prompt
- Writes `<subagent.name>.md` for each subagent with just the prompt content
- Returns path to created candidate directory

### 6. Create train command

**File**: `src/lw_coder/train_command.py`

Main orchestration:

```python
def run_train_command(
    batch_size: int = 3,
    max_subagents: int = 5,
    model: str = "sonnet",
) -> int:
    """Run the train command to generate a candidate prompt set."""
```

Steps:
1. Find repo root
2. Load training batch
3. Load current prompts from `prompts/active/`
4. Run DSPy prompt trainer
5. Write candidate to `prompts/candidates/`
6. Report token usage and candidate location
7. Return exit code

### 7. Update init command for new directory structure

**File**: `src/lw_coder/init_command.py`

Update to install prompts to `prompts/active/` instead of `optimized_prompts/`.

### 8. Add CLI integration

**File**: `src/lw_coder/cli.py`

Add train command:
```
lw_coder train [--batch-size N] [--max-subagents N] [--model MODEL]
```

Arguments:
- `--batch-size`: Number of training samples per batch (default: 3)
- `--max-subagents`: Maximum subagents to generate (default: 5)
- `--model`: Model for DSPy calls (default: "sonnet")

### 9. Add tab completion for train command

**File**: `src/lw_coder/tab_completion.py`

Add `train` to the list of commands for tab completion. The train command has simple flag arguments (`--batch-size`, `--max-subagents`, `--model`) that don't require custom completers.

### 10. Update README documentation

**File**: `README.md`

Add documentation for the train command:
- Command syntax and arguments
- What it does (analyzes training data, generates candidate prompts)
- Prerequisites (training data from `eval` command, active prompts)
- Example usage
- Where candidates are saved

## Deliverables

1. **New Files**:
   - `src/lw_coder/training_types.py` - Pydantic models
   - `src/lw_coder/training_data_loader.py` - Load training samples
   - `src/lw_coder/prompt_trainer.py` - DSPy module
   - `src/lw_coder/candidate_writer.py` - Write candidate prompts
   - `src/lw_coder/train_command.py` - Command orchestration

2. **Modified Files**:
   - `src/lw_coder/prompt_loader.py` - New directory structure + migration
   - `src/lw_coder/init_command.py` - Install to new location
   - `src/lw_coder/cli.py` - Add train command
   - `src/lw_coder/tab_completion.py` - Add train command completion
   - `README.md` - Document train command

3. **Tests**:
   - Unit tests for all new modules
   - Integration tests for DSPy prompt generation

## Out of Scope

- Droid platform support (Claude Code only)
- Auto-promotion of candidates to active
- Manifest.json or metadata files
- `original/` baseline directory
- Multi-batch training in single run
- Parallel training runs
- Candidate comparison or ranking
- Rollback mechanisms

## Unit Tests

**File**: `tests/unit/test_training_types.py`

1. `test_training_sample_validation` - TrainingSample validates required fields
2. `test_subagent_definition_validation` - SubagentDefinition validates name/description/prompt
3. `test_candidate_prompts_subagent_list` - CandidatePrompts accepts list of subagents

**File**: `tests/unit/test_training_data_loader.py`

1. `test_discover_training_samples` - Finds plan_ids in training_data directory
2. `test_load_training_sample_success` - Loads complete sample with all files
3. `test_load_training_sample_missing_required` - Raises error for missing human_feedback.md
4. `test_load_training_sample_optional_missing` - Succeeds when code_trace.md missing
5. `test_load_training_batch_respects_limit` - Only loads batch_size samples
6. `test_load_training_batch_empty_directory` - Raises error when no samples

**File**: `tests/unit/test_prompt_loader_migration.py`

1. `test_load_prompts_from_new_location` - Loads from prompts/active/
2. `test_load_prompts_migrates_old_location` - Migrates optimized_prompts/ to prompts/active/
3. `test_load_prompts_deletes_old_after_migration` - Old directory removed after migration
4. `test_load_prompts_no_double_migration` - Doesn't migrate if already migrated
5. `test_load_current_prompts_for_training` - Returns CurrentPrompts object with subagents

**File**: `tests/unit/test_candidate_writer.py`

1. `test_get_next_candidate_number_empty` - Returns 1 when no candidates exist
2. `test_get_next_candidate_number_sequential` - Returns next number after existing
3. `test_write_candidate_creates_directory` - Creates candidate-NNN directory
4. `test_write_candidate_writes_main_prompt` - Writes main.md
5. `test_write_candidate_writes_subagents` - Writes subagent .md files
6. `test_write_candidate_returns_path` - Returns path to created directory

**File**: `tests/unit/test_train_command.py`

1. `test_train_command_validates_batch_size` - Rejects invalid batch size
2. `test_train_command_validates_max_subagents` - Rejects invalid max_subagents
3. `test_train_command_no_training_data` - Returns error when no training data
4. `test_train_command_no_active_prompts` - Returns error when no active prompts

## Integration Tests

**File**: `tests/integration/test_train_command_integration.py`

### `test_train_command_end_to_end`

Full workflow with one batch covering the complete happy path. No mocks for the DSPy/LLM call—this uses real API calls with caching per project guidelines.

**Test Setup:**

1. Create a temporary directory structure matching real training data format:
   ```
   tmp/
   ├── .lw_coder/
   │   ├── prompts/
   │   │   └── active/
   │   │       └── claude-code-cli/
   │   │           └── sonnet/
   │   │               ├── main.md                    # Simple test prompt
   │   │               ├── code-review-auditor.md     # Existing subagent
   │   │               └── plan-alignment-checker.md  # Existing subagent
   │   └── training_data/
   │       └── test-plan-001/
   │           ├── plan.md                      # Minimal plan content
   │           ├── code_trace.md                # Conversation trace (can be minimal)
   │           ├── human_feedback.md            # "Agent skipped tests"
   │           ├── test_results_before.json     # {"passed": 10, "failed": 0}
   │           ├── test_results_after.json      # {"passed": 9, "failed": 1}
   │           ├── judge_code-reuse.json        # {"judge_name": "code-reuse", "weight": 0.4, "score": 0.8, "feedback": "..."}
   │           ├── judge_code-reuse.md          # Markdown version of feedback
   │           ├── judge_plan-compliance.json   # {"judge_name": "plan-compliance", "weight": 0.6, "score": 0.7, "feedback": "..."}
   │           └── judge_plan-compliance.md     # Markdown version of feedback
   ```

2. Files match TrainingSample Pydantic model:
   - `plan_id`: derived from directory name
   - `plan_content`: from plan.md
   - `code_trace`: from code_trace.md
   - `human_feedback`: from human_feedback.md
   - `judge_results`: formatted from judge_*.json files
   - `test_results_before`: from test_results_before.json
   - `test_results_after`: from test_results_after.json

3. Use real (minimal) content for training data that will produce a predictable analysis direction

**Test Execution:**

1. Call `run_train_command(batch_size=1, max_subagents=3, model="sonnet")` with repo_root set to tmp directory
2. No mocks on DSPy—let it make real API call (cached after first run)

**Assertions (structure, not content):**

1. **Return code**: Command returns 0 (success)

2. **Candidate directory created**:
   - Path exists: `tmp/.lw_coder/prompts/candidates/claude-code-cli/sonnet/candidate-001/`

3. **Main prompt written**:
   - File exists: `candidate-001/main.md`
   - Content is non-empty string
   - Content is valid markdown (no JSON parsing errors, etc.)

4. **Subagent files written**:
   - At least 1 subagent file exists (e.g., `*.md` files other than main.md)
   - No more than 3 subagent files (respects max_subagents=3)
   - Each subagent file is non-empty

5. **Token usage reported**:
   - Function returns token usage dict (or logs it)
   - Dict has keys: `input_tokens`, `output_tokens`, `total_tokens`
   - All values are positive integers

6. **Analysis summary exists**:
   - The CandidatePrompts.analysis_summary is non-empty
   - (We don't assert content, just that it exists)

**What we intentionally DON'T assert:**

- Specific prompt wording or improvements
- Exact number of subagents (just within bounds)
- Specific subagent names
- Exact token counts
- That the prompts are "better" than input

**Why no mocks for intermediate steps:**

- DSPy caching handles repeated runs efficiently
- Mocking would defeat the purpose of integration testing
- We want to verify the real LLM produces valid structured output
- The Pydantic models validate structure; the test verifies integration

## Tests NOT to Write

Per project guidelines and to avoid test bloat:

1. **Don't test prompt quality or content** - LLM outputs are non-deterministic; verify structure, not wording
2. **Don't test exact token counts** - Token counts vary between API calls; verify fields exist and are positive integers
3. **Don't mock DSPy in integration tests** - Per CLAUDE.md, use real DSPy with real API calls and caching
4. **Don't test that DSPy framework works** - Trust external libraries; test our usage of them
5. **Don't test interactive command execution** - Don't run `lw_coder train` as subprocess; test `run_train_command()` directly
6. **Don't test README documentation exists** - Documentation verified manually per guidelines
